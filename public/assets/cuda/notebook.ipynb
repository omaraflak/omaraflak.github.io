{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%%writefile cuda_check.cu\n",
        "\n",
        "#include <iostream>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "int main() {\n",
        "    int deviceCount;\n",
        "    cudaGetDeviceCount(&deviceCount);\n",
        "\n",
        "    if (deviceCount == 0) {\n",
        "        std::cerr << \"No CUDA devices found.\" << std::endl;\n",
        "        return 1;\n",
        "    }\n",
        "\n",
        "    for (int i = 0; i < deviceCount; ++i) {\n",
        "        cudaDeviceProp prop;\n",
        "        cudaGetDeviceProperties(&prop, i);\n",
        "\n",
        "        std::cout << \"# Device \" << i << \": \" << prop.name << std::endl;\n",
        "        std::cout << \"#   Compute capability: \" << prop.major << \".\" << prop.minor << std::endl;\n",
        "        std::cout << \"#   Total global memory: \" << prop.totalGlobalMem / (1024 * 1024) << \" MB\" << std::endl;\n",
        "        std::cout << \"#   Max threads per block: \" << prop.maxThreadsPerBlock << std::endl;\n",
        "        std::cout << \"#   Max threads per multiprocessor: \" << prop.maxThreadsPerMultiProcessor << std::endl;\n",
        "        std::cout << \"#   Number of multiprocessors: \" << prop.multiProcessorCount << std::endl;\n",
        "        std::cout << \"#   Shared memory per block: \" << prop.sharedMemPerBlock / 1024 << \" KB\" << std::endl;\n",
        "        std::cout << \"#   Registers per block: \" << prop.regsPerBlock << std::endl;\n",
        "        std::cout << \"#   Warp size: \" << prop.warpSize << std::endl;\n",
        "        std::cout << \"#   Max block dimensions: [\" << prop.maxThreadsDim[0] << \", \" << prop.maxThreadsDim[1] << \", \" << prop.maxThreadsDim[2] << \"]\" << std::endl;\n",
        "        std::cout << \"#   Max grid dimensions: [\" << prop.maxGridSize[0] << \", \" << prop.maxGridSize[1] << \", \" << prop.maxGridSize[2] << \"]\" << std::endl << std::endl;\n",
        "    }\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sjUbpedibg0I",
        "outputId": "c389e375-24ac-4ea8-c296-b86dbcec3abb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing cuda_check.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc cuda_check.cu -o cuda_check && ./cuda_check"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hZS8Y3ZnbhaD",
        "outputId": "be42d82d-fd6a-4f5f-e11d-1b1e989a0f35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Device 0: Tesla T4\n",
            "#   Compute capability: 7.5\n",
            "#   Total global memory: 15095 MB\n",
            "#   Max threads per block: 1024\n",
            "#   Max threads per multiprocessor: 1024\n",
            "#   Number of multiprocessors: 40\n",
            "#   Shared memory per block: 48 KB\n",
            "#   Registers per block: 65536\n",
            "#   Warp size: 32\n",
            "#   Max block dimensions: [1024, 1024, 64]\n",
            "#   Max grid dimensions: [2147483647, 65535, 65535]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile main.cu\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "__global__ void add_kernel(float *array1, float *array2, float *result, int size)\n",
        "{\n",
        "    int start = blockDim.x * blockIdx.x + threadIdx.x;\n",
        "    int stride = gridDim.x * blockDim.x;\n",
        "\n",
        "    for (int i=start; i<size; i+=stride) {\n",
        "        result[i] = array1[i] + array2[i];\n",
        "    }\n",
        "}\n",
        "\n",
        "void add(float *array1, float *array2, float *result, int size)\n",
        "{\n",
        "    float *cuda_array1;\n",
        "    float *cuda_array2;\n",
        "    float *cuda_result;\n",
        "\n",
        "    // Allocate memory on GPU\n",
        "    cudaMalloc(&cuda_array1, sizeof(float) * size);\n",
        "    cudaMalloc(&cuda_array2, sizeof(float) * size);\n",
        "    cudaMalloc(&cuda_result, sizeof(float) * size);\n",
        "\n",
        "    // Copy array1 and array2 from CPU to GPU\n",
        "    cudaMemcpy(cuda_array1, array1, sizeof(float) * size, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(cuda_array2, array2, sizeof(float) * size, cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Run the kernel\n",
        "    int block_dim = 1024;\n",
        "    int grid_dim = (size + block_dim - 1) / block_dim;\n",
        "    add_kernel<<<grid_dim, block_dim>>>(cuda_array1, cuda_array2, cuda_result, size);\n",
        "\n",
        "    // Wait for the kernel to finish\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    // Copy cuda_result from GPU to CPU\n",
        "    cudaMemcpy(result, cuda_result, sizeof(float) * size, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // Free GPU memory\n",
        "    cudaFree(cuda_array1);\n",
        "    cudaFree(cuda_array2);\n",
        "    cudaFree(cuda_result);\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int size = 1 << 20;\n",
        "    float *array1 = (float*) malloc(sizeof(float) * size);\n",
        "    float *array2 = (float*) malloc(sizeof(float) * size);\n",
        "    float *result = (float*) malloc(sizeof(float) * size);\n",
        "\n",
        "    for (int i = 0; i < size; i++)\n",
        "    {\n",
        "        array1[i] = 2;\n",
        "        array2[i] = 3;\n",
        "    }\n",
        "\n",
        "    add(array1, array2, result, size);\n",
        "\n",
        "    int errors = 0;\n",
        "    for (int i = 0; i < size; i++)\n",
        "    {\n",
        "        if (result[i] != 5)\n",
        "        {\n",
        "            errors++;\n",
        "        }\n",
        "    }\n",
        "    printf(\"Errors = %d\\n\", errors);\n",
        "\n",
        "    free(array1);\n",
        "    free(array2);\n",
        "    free(result);\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xvp2cGRNro4B",
        "outputId": "0d617b3b-b023-4658-a988-33eb0b19e870"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing main.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc main.cu -o main -arch=sm_75 && nvprof ./main"
      ],
      "metadata": {
        "id": "2GaYG8-1FcZd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b4d2132-6c08-4da1-826e-7b117e924c70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==957== NVPROF is profiling process 957, command: ./main\n",
            "Errors = 0\n",
            "==957== Profiling application: ./main\n",
            "==957== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   52.62%  1.7883ms         1  1.7883ms  1.7883ms  1.7883ms  [CUDA memcpy DtoH]\n",
            "                   45.14%  1.5340ms         2  767.02us  766.49us  767.55us  [CUDA memcpy HtoD]\n",
            "                    2.23%  75.936us         1  75.936us  75.936us  75.936us  add_kernel(float*, float*, float*, int)\n",
            "      API calls:   96.66%  227.84ms         3  75.948ms  88.745us  227.65ms  cudaMalloc\n",
            "                    2.57%  6.0637ms         3  2.0212ms  988.06us  3.3135ms  cudaMemcpy\n",
            "                    0.34%  793.50us         1  793.50us  793.50us  793.50us  cuDeviceGetPCIBusId\n",
            "                    0.24%  575.07us         3  191.69us  136.39us  223.11us  cudaFree\n",
            "                    0.08%  184.58us       114  1.6190us     181ns  74.374us  cuDeviceGetAttribute\n",
            "                    0.07%  167.22us         1  167.22us  167.22us  167.22us  cudaLaunchKernel\n",
            "                    0.03%  77.561us         1  77.561us  77.561us  77.561us  cudaDeviceSynchronize\n",
            "                    0.01%  12.354us         1  12.354us  12.354us  12.354us  cuDeviceGetName\n",
            "                    0.00%  1.8110us         3     603ns     222ns  1.3390us  cuDeviceGetCount\n",
            "                    0.00%     908ns         2     454ns     277ns     631ns  cuDeviceGet\n",
            "                    0.00%     637ns         1     637ns     637ns     637ns  cuDeviceTotalMem\n",
            "                    0.00%     550ns         1     550ns     550ns     550ns  cuModuleGetLoadingMode\n",
            "                    0.00%     348ns         1     348ns     348ns     348ns  cuDeviceGetUuid\n"
          ]
        }
      ]
    }
  ]
}