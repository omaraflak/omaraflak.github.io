<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta property="og:title" content="CUDA Kernels">
    <meta property="og:description" content="Gain a 1650x speed up by writing CUDA kernels">
    <title>CUDA Kernels</title>
    <link rel="icon" type="image/svg+xml" href="/images/favicon.svg">
    <link rel="stylesheet" href="../styles/index.css">
    <link rel="stylesheet" href="../styles/fonts.css">
    
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css" integrity="sha512-fHwaWebuwA7NSF5Qg/af4UeDx9XqUpYpOGgubo3yWu+b2IQR4UeQwbb42Ti7gVAjNtVoI/I9TEoYeu9omwcC6g==" crossorigin="anonymous" referrerpolicy="no-referrer" />


<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.11.1/styles/github.min.css" integrity="sha512-0aPQyyeZrWj9sCA46UlmWgKOP0mUipLQ6OZXu8l4IcAmD2u31EPEy9VcIMvl7SoAaKe8bLXZhYoMaE/in+gcgA==" crossorigin="anonymous" referrerpolicy="no-referrer" />

</head>

<body>
    <div class="site-header">
        <div class="site-header-content">
            <a href="/index.html" class="site-header-link">Home</a>
            <a href="/articles.html" class="site-header-link">Blog</a>
        </div>
    </div>
    <div class="article-header">
        <div class="article-header-content">
            <h1 class="article-title">CUDA Kernels</h1>
            <p class="article-description">Gain a 1650x speed up by writing CUDA kernels</p>
            <p class="article-date">Aug 19, 2025</p>
            
        </div>
    </div>
    <article class="article-body">
        <p>When I first learned about GPU programming with CUDA I was very confused with certain terms like <span class="article-italic">kernels</span>, <span class="article-italic">thread blocks</span>, and <span class="article-italic">grids</span>, or even just <span class="article-italic">how</span> does code run on the GPU.</p>
<p>In this article we'll go through the fundamentals of CUDA and write code that runs on GPU. We'll use <span class="article-italic">Google Colab</span> to run our code, but of course you can use your own CUDA device if you have one.</p>
<h2 id=cuda-in-google-colab class="article-section">CUDA In Google Colab</h2>
<p>To run GPU code, you must have the CUDA libraries installed. Google Colab gives you everything out of the box for free.</p>
<p>Create a new notebook in <a class="article-link" target="_blank" href="https://colab.research.google.com">Google Colab</a> and change the runtime using the top-right menu:</p>
<center><img class="article-image" height="" width="100%" src="/assets/cuda/colab.png" alt="Google Colab runtime"></center>
<p>Select any runtime with a GPU or TPU. In my case I selected "T4 GPU".</p>
<p>Colab has a special instruction <code class="article-code-inline">%%writefile filename</code> which you can put at the beginning of a cell. Such cells, when executed, will write their content in a file named <code class="article-code-inline">filename</code>.</p>
<p>Create a first cell:</p>
<pre class="article-code-block cpp"><code>%%writefile cuda_check.cu

#include &lt;stdio.h&gt;
#include &lt;cuda_runtime.h&gt;

int main() {
    int deviceCount;
    cudaGetDeviceCount(&amp;deviceCount);

    if (deviceCount == 0) {
        fprintf(stderr, &quot;No CUDA devices found.\n&quot;);
        return 1;
    }

    for (int i = 0; i &lt; deviceCount; ++i) {
        cudaDeviceProp prop;
        cudaGetDeviceProperties(&amp;prop, i);

        printf(&quot;# Device %d: %s\n&quot;, i, prop.name);
        printf(&quot;#   Compute capability: %d.%d\n&quot;, prop.major, prop.minor);
        printf(&quot;#   Total global memory: %lld MB\n&quot;, (long long) prop.totalGlobalMem / (1024 * 1024));
        printf(&quot;#   Max threads per block: %d\n&quot;, prop.maxThreadsPerBlock);
        printf(&quot;#   Max threads per multiprocessor: %d\n&quot;, prop.maxThreadsPerMultiProcessor);
        printf(&quot;#   Number of multiprocessors: %d\n&quot;, prop.multiProcessorCount);
        printf(&quot;#   Shared memory per block: %lld KB\n&quot;, (long long) prop.sharedMemPerBlock / 1024);
        printf(&quot;#   Registers per block: %d\n&quot;, prop.regsPerBlock);
        printf(&quot;#   Warp size: %d\n&quot;, prop.warpSize);
        printf(&quot;#   Max block dimensions: [%d, %d, %d]\n&quot;, prop.maxThreadsDim[0], prop.maxThreadsDim[1], prop.maxThreadsDim[2]);
        printf(&quot;#   Max grid dimensions: [%d, %d, %d]\n\n&quot;, prop.maxGridSize[0], prop.maxGridSize[1], prop.maxGridSize[2]);
    }

    return 0;
}</code></pre>
<p>In a second cell, compile and run the file:</p>
<pre class="article-code-block shell"><code>!nvcc cuda_check.cu -o cuda_check &amp;&amp; ./cuda_check</code></pre>
<div class="article-quote">Mind the <code class="article-code-inline">!</code> at the beginning of the line.</div>
<p>Run both cells and you should see various GPU specs printed out. You're all set!</p>
<h2 id=kernels-threads-blocks-grids class="article-section">Kernels, Threads, Blocks, Grids</h2>
<p>A <span class="article-bold"><span class="article-italic">kernel</span></span> is nothing more but a <span class="article-bold">function</span> that runs on the GPU. CUDA allows you to write those functions in C/C++ and it gives you control over the parallelization.</p>
<p>A <span class="article-bold"><span class="article-italic">thread</span></span> is like a mini process that runs an <span class="article-bold"><span class="article-italic">instance</span></span> of your kernel (more on that later), it is ultimately what runs your code. A GPU may have thousands of threads running in parallel.</p>
<p>A <span class="article-bold"><span class="article-italic">block</span></span> is a collection of threads which have a shared memory space and that you can synchronize if needed (let the threads wait for each other to complete).</p>
<p>A <span class="article-bold"><span class="article-italic">grid</span></span> is a collection of blocks, however threads in different blocks don't share any memory space and cannot be synchronized.</p>
<p>A kernel always runs in a <span class="article-bold"><span class="article-italic">single grid</span></span>, but you get to decide how many <span class="article-italic">blocks</span> that grid contains, and how many <span class="article-italic">threads</span> each of those blocks contain.</p>
<h3 id=gpu-specs class="article-small-section">GPU Specs</h3>
<p>Running the code above, I get the following output:</p>
<pre class="article-code-block shell"><code># Device 0: Tesla T4
#   Compute capability: 7.5
#   Total global memory: 15095 MB
#   Max threads per block: 1024
#   Max threads per multiprocessor: 1024
#   Number of multiprocessors: 40
#   Shared memory per block: 48 KB
#   Registers per block: 65536
#   Warp size: 32
#   Max block dimensions: [1024, 1024, 64]
#   Max grid dimensions: [2147483647, 65535, 65535]</code></pre>
<p>Some comments:</p>
<ul><li class="article-li"><code class="article-code-inline">Compute capability: 7.5</code> is important: when running kernels later, you'll need to compile the code with <code class="article-code-inline">nvcc -arch=sm_75</code>. Modify the flag accordingly.</li>
<li class="article-li"><code class="article-code-inline">Max threads per multiprocessor: 1024</code> refers to the number of <span class="article-bold"><span class="article-italic">Streaming Multiprocessors</span></span>, or SMs. Those are the GPU cores. A streaming multiprocessor is what picks on up on <span class="article-bold">blocks</span> and runs the threads in them. One SM does not necessarily pick up all blocks of a grid, which is why threads in a single block share memory, but not threads across multiple blocks. This GPU has <code class="article-code-inline">40</code> multiprocessors that can process <code class="article-code-inline">1024</code> threads each. That is a total of <code class="article-code-inline">40,960</code> concurrent threads!</li>
<li class="article-li"><code class="article-code-inline">Warp size: 32</code>: when a block is executed on an SM, it is divided into <span class="article-bold"><span class="article-italic">warps</span></span>, and it's the warp that is actually sent to the SM. This GPU has <code class="article-code-inline">32</code> threads per warp. The <span class="article-italic">warp scheduler</span> is responsible for picking up threads to run, and send them as a warp to the SM. That said, this is more of a hardware detail.</li>
<li class="article-li">We'll get back to <code class="article-code-inline">Max block dimensions: [1024, 1024, 64]</code> and <code class="article-code-inline">Max grid dimensions: [2147483647, 65535, 65535]</code> later.</li></ul>
<p>For now, the single most important number is <code class="article-code-inline">Max threads per block: 1024</code>.</p>
<h2 id=overview class="article-section">Overview</h2>
<p>Before we bury our head in the code, I'd like to explain the big picture.</p>
<p>In GPU programming, the kernel (function) we write is going to be called <span class="article-bold">many times</span> by different threads; a single call to the function is expected to process <span class="article-bold">a subset</span> of our problem.</p>
<p>Let's supposed we want to add together two arrays of 1 million elements each. You <span class="article-bold">won't</span> write a loop over 1 million elements directly. Instead, you might design the kernel to process 2 elements per call, and let CUDA call your function 500,000 times. Thus, each call to the kernel must be able to determine <span class="article-bold">which elements to process</span>.</p>
<p>The GPU has its own memory. This means data must be moved/copied to and fro the CPU/GPU. This also means we'll have to deal with pointers to CPU addresses and pointers to GPU addresses, and we must not try to dereference (access) those pointers on the wrong device!</p>
<p>In a nutshell, if we wanted to add 2 arrays on the GPU, we would:</p>
<ul><li class="article-li">Allocate and fill the arrays on the CPU</li>
<li class="article-li">Allocate memory on the GPU</li>
<li class="article-li">Copy the arrays from the CPU to the GPU</li>
<li class="article-li">Run the kernel</li>
<li class="article-li">Copy the result from the GPU to the CPU</li>
<li class="article-li">Free the GPU memory</li>
<li class="article-li">Free the CPU memory</li></ul>
<h2 id=writing-a-kernel class="article-section">Writing A Kernel</h2>
<p>We'll now write our first kernel. Let's continue with the example we took earlier, adding two arrays together:</p>
<pre class="article-code-block cpp"><code>#include &lt;stdio.h&gt;
#include &lt;cuda_runtime.h&gt;

__global__ void add_kernel(float *array1, float *array2, float *result, int size)
{
    int i = blockDim.x * blockIdx.x + threadIdx.x;
    if (i &lt; size) {
        result[i] = array1[i] + array2[i];
    }
}

void add(float *array1, float *array2, float *result, int size)
{
    float *cuda_array1;
    float *cuda_array2;
    float *cuda_result;

    // Allocate memory on GPU
    cudaMalloc(&amp;cuda_array1, sizeof(float) * size);
    cudaMalloc(&amp;cuda_array2, sizeof(float) * size);
    cudaMalloc(&amp;cuda_result, sizeof(float) * size);

    // Copy array1 and array2 from CPU to GPU
    cudaMemcpy(cuda_array1, array1, sizeof(float) * size, cudaMemcpyHostToDevice);
    cudaMemcpy(cuda_array2, array2, sizeof(float) * size, cudaMemcpyHostToDevice);

    // Run the kernel
    dim3 block_dim(size);
    dim3 grid_dim(1);
    add_kernel&lt;&lt;&lt;grid_dim, block_dim&gt;&gt;&gt;(cuda_array1, cuda_array2, cuda_result, size);

    // Wait for the kernel to finish
    cudaDeviceSynchronize();

    // Copy cuda_result from GPU to CPU
    cudaMemcpy(result, cuda_result, sizeof(float) * size, cudaMemcpyDeviceToHost);

    // Free GPU memory
    cudaFree(cuda_array1);
    cudaFree(cuda_array2);
    cudaFree(cuda_result);
}

int main() {
    int size = 1000;
    float *array1 = (float*) malloc(sizeof(float) * size);
    float *array2 = (float*) malloc(sizeof(float) * size);
    float *result = (float*) malloc(sizeof(float) * size);

    for (int i = 0; i &lt; size; i++)
    {
        array1[i] = 2;
        array2[i] = 3;
    }

    add(array1, array2, result, size);

    int errors = 0;
    for (int i = 0; i &lt; size; i++)
    {
        if (result[i] != 5)
        {
            errors++;
        }
    }
    printf(&quot;Errors = %d\n&quot;, errors);

    free(array1);
    free(array2);
    free(result);
}</code></pre>
<div class="article-quote">I compiled and ran this with <code class="article-code-inline">nvcc -arch=sm_75 main.cu -o main &amp;&amp; ./main</code>.</div>
<p>There's a lot of boilerplate, but the important bits are the kernel itself and the call to the kernel.</p>
<div class="article-quote">What are those <code class="article-code-inline">__global__</code>, <code class="article-code-inline">blockDim</code>, <code class="article-code-inline">blockIdx</code>, <code class="article-code-inline">gridDim</code>, <code class="article-code-inline">&lt;&lt;&lt;...&gt;&gt;&gt;</code> ?</div>
<p>First, <code class="article-code-inline">__global__</code> is how we define a kernel in CUDA. The function <code class="article-code-inline">add_kernel</code> will now be executed on the GPU. As I said earlier, the GPU has its own memory which you need to allocate using <code class="article-code-inline">cudaMalloc</code> (and free using <code class="article-code-inline">cudaFree</code>) and to which you can copy data with <code class="article-code-inline">cudaMemcpy</code>. When the data is processed and you want to read it, you need to move it from the GPU <span class="article-bold">back</span> to the CPU. This is most of the boilerplate which I commented.</p>
<h3 id=kernel-call class="article-small-section">Kernel Call</h3>
<pre class="article-code-block cpp"><code>dim3 block_dim(size);
dim3 grid_dim(1);
add_kernel&lt;&lt;&lt;grid_dim, block_dim&gt;&gt;&gt;(cuda_array1, cuda_array2, cuda_result, size);</code></pre>
<p>When calling the kernel, we use the CUDA triple chevron syntax <code class="article-code-inline">&lt;&lt;&lt;...&gt;&gt;&gt;</code>. This syntax takes 2 arguments:</p>
<ul><li class="article-li"><code class="article-code-inline">grid_dim</code>: the number of blocks in the grid executing the kernel</li>
<li class="article-li"><code class="article-code-inline">block_dim</code>: the number of threads in each of those blocks</li></ul>
<p>Those two variables are of type <code class="article-code-inline">dim3</code> which can take 3 arguments (<code class="article-code-inline">x</code>, <code class="article-code-inline">y</code>, <code class="article-code-inline">z</code>). When specifying a single number, it is assigned to <code class="article-code-inline">x</code> (in that case you could pass an <code class="article-code-inline">int</code> directly). For now ignore those 3 dimensions, and do as if it was a single number, we will get back to this at the end.</p>
<div class="article-quote">In essence, you are giving the dimension of a matrix of <code class="article-code-inline">grid_dim</code> rows and <code class="article-code-inline">block_dim</code> columns, containing the threads that will run the kernel.</div>
<p>So here, we are running the kernel on a single block which has <code class="article-code-inline">size=1000</code> threads. Note that our GPU has a maximum of <code class="article-code-inline">1024</code> threads per block, so this will fail if <code class="article-code-inline">size &gt; 1024</code>. We will see how to deal with that later.</p>
<p>Remember that I wrote earlier: a thread executes an <span class="article-bold"><span class="article-italic">instance</span></span> of your kernel. So <code class="article-code-inline">add_kernel&lt;&lt;&lt;grid_dim, block_dim&gt;&gt;&gt;(...)</code> will in fact trigger <code class="article-code-inline">grid_dim * block_dim = 1 * 1000 = 1000</code> calls to your function, one from each thread.</p>
<h3 id=threadidx-blockidx-blockdim class="article-small-section">threadIdx, blockIdx, blockDim</h3>
<pre class="article-code-block cpp"><code>__global__ void add_kernel(float *array1, float *array2, float *result, int size)
{
    int i = blockDim.x * blockIdx.x + threadIdx.x;
    if (i &lt; size) {
        result[i] = array1[i] + array2[i];
    }
}</code></pre>
<p>In the kernel you can see those special variables not declared anywhere. Those are <code class="article-code-inline">dim3</code> objects like <code class="article-code-inline">block_dim</code> and <code class="article-code-inline">grid_dim</code>, and they are given to us by CUDA. Those variables give us information on <span class="article-bold"><span class="article-italic">which thread</span></span> is running the kernel <span class="article-italic">instance</span>.</p>
<ul><li class="article-li"><code class="article-code-inline">threadIdx</code> is the index of the thread (running the kernel) in the block it belongs to</li>
<li class="article-li"><code class="article-code-inline">blockIdx</code> is the index of the block (running the thread) in the grid it belongs to</li>
<li class="article-li"><code class="article-code-inline">blockDim</code> is the number of threads in the block, which is what we passed to the kernel call as <code class="article-code-inline">block_dim</code></li></ul>
<div class="article-quote"><code class="article-code-inline">(blockIdx, threadIdx)</code> constitutes the <code class="article-code-inline">(row, col)</code> position in the matrix of the thread running the kernel.</div>
<style>
    .grid-container {
        display: block;
        overflow-x: scroll;
        width: 100%;
        margin-top: 20px;
        margin-bottom: 20px;
    }

    .grid {
        width: 100%;
        display: grid;
        grid-template-columns: repeat(4, 1fr);
        gap: 5px;
    }

    .thread {
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        border-radius: 8px;
        border: 1px solid #333333;
        padding: 10px;
        font-size: 12px;
        font-family: 'Courier New', Courier, monospace;
        font-weight: bold;
    }

    .emphasis {
        border: 2px solid #f64a4a;
        color: #f64a4a;
    }
</style>

<div class="grid-container">
    <div class="grid">
        <div class="thread"><span>blockIdx=0</span><span>threadIdx=0</span></div>
        <div class="thread"><span>blockIdx=0</span><span>threadIdx=1</span></div>
        <div class="thread"><span>blockIdx=0</span><span>threadIdx=2</span></div>
        <div class="thread"><span>blockIdx=0</span><span>threadIdx=3</span></div>
        <div class="thread"><span>blockIdx=1</span><span>threadIdx=0</span></div>
        <div class="thread"><span>blockIdx=1</span><span>threadIdx=1</span></div>
        <div class="thread emphasis"><span>blockIdx=1</span><span>threadIdx=2</span></div>
        <div class="thread"><span>blockIdx=1</span><span>threadIdx=3</span></div>
        <div class="thread"><span>blockIdx=2</span><span>threadIdx=0</span></div>
        <div class="thread"><span>blockIdx=2</span><span>threadIdx=1</span></div>
        <div class="thread"><span>blockIdx=2</span><span>threadIdx=2</span></div>
        <div class="thread"><span>blockIdx=2</span><span>threadIdx=3</span></div>
    </div>
</div>
<p>This is a representation of the threads running your kernel if you had called it with <code class="article-code-inline">add_kernel&lt;&lt;&lt;3, 4&gt;&gt;&gt;(...)</code> (3 blocks of 4 threads, so 12 threads in total). Each row represents a <span class="article-italic">block</span>, and each cell in a block is a <span class="article-italic">thread</span> running your kernel.</p>
<p>We can assign a global index to each thread by flattening the grid in row-major order. The thread in position <code class="article-code-inline">(1, 2)</code> has a flat index of <code class="article-code-inline">4 * 1 + 2 = 6</code>.</p>
<p>This is exactly what you see in the kernel code:</p>
<pre class="article-code-block cpp"><code>int i = blockDim.x * blockIdx.x + threadIdx.x;</code></pre>
<p>In our code however, <code class="article-code-inline">blockDim=1</code> therefore <code class="article-code-inline">blockIdx=0</code>, so this simplifies to <code class="article-code-inline">1 * 0 + threadIdx = threadIdx</code>.</p>
<h3 id=why-1-1000-or-3-4 class="article-small-section">Why <<<1, 1000>>> or <<<3, 4>>> ?</h3>
<p>So this triple chevron syntax is in fact just specifying the size of a matrix (grid). A matrix of threads that will run the kernel.</p>
<div class="article-quote">How to choose the dimension of the matrix?</div>
<p>It depends on the problem. If you have 1000 elements to process, you can use a single block of 1000 threads <code class="article-code-inline">&lt;&lt;&lt;1, 1000&gt;&gt;&gt;</code>. But remember each block has a limit of 1024 threads. If you wanted to process 2000 elements at once, you would have to split those over 2 blocks. For example by using 2 blocks of 1000 threads <code class="article-code-inline">&lt;&lt;&lt;2, 1000&gt;&gt;&gt;</code>.</p>
<h3 id=what-if-we-have-more-threads-than-elements-to-process class="article-small-section">What if we have more threads than elements to process ?</h3>
<p>Say you want to process 1049 elements (that's a prime number). The best you can do is start the kernel with <code class="article-code-inline">&lt;&lt;&lt;2, 525&gt;&gt;&gt;</code>. This will leave a single <span class="article-bold">unused</span> thread (1050 threads vs 1049 elements to process).</p>
<p>Of course, since CUDA doesn't know this thread is unused, it will run regardless. It is your job to make it a no-operation. This is why we have this line in the kernel:</p>
<pre class="article-code-block cpp"><code>if (i &lt; size) {
    // add arrays
}</code></pre>
<h3 id=what-if-we-have-more-elements-to-process-than-threads class="article-small-section">What if we have more elements to process than threads ?</h3>
<p>As we saw earlier, our GPU has about 40k threads that can run in parallel. How can we process, say, 100k elements? You don't even need to get to that extreme, you might simply want to split the GPU usage across different calculations, so you'll have a certain number of threads allocated to a certain task.</p>
<div class="article-quote">Vertical scaling!</div>
<p>A thread doesn't have to process only a single element, it can process multiple. For example, if the GPU had 1000 threads available and we wanted to process 5000 elements, we could run 1000 threads and have each thread process 5 elements.</p>
<p>A common way to do that is to use a <span class="article-italic">grid-stride loop</span>:</p>
<ul><li class="article-li">Thread <code class="article-code-inline">0</code> will process elements <code class="article-code-inline">0, 1000, 2000, 3000, 4000</code></li>
<li class="article-li">Thread <code class="article-code-inline">1</code> will process elements <code class="article-code-inline">1, 1001, 2001, 3001, 4001</code></li>
<li class="article-li">Thread <code class="article-code-inline">999</code> will process elements <code class="article-code-inline">999, 1999, 2999, 3999, 4999</code></li></ul>
<pre class="article-code-block cpp"><code>__global__ void add_kernel(float *array1, float *array2, float *result, int size)
{
    int start = blockDim.x * blockIdx.x + threadIdx.x;
    int stride = gridDim.x * blockDim.x;

    for (int i=start; i&lt;size; i+=stride) {
        result[i] = array1[i] + array2[i];
    }
}

// ... for the sake of the example, suppose the maximum block size is 500, so we use 2 thread blocks
add_kernel&lt;&lt;&lt;2, 500&gt;&gt;&gt;(cuda_array1, cuda_array2, cuda_result, 5000);</code></pre>
<div class="article-quote">Where is <code class="article-code-inline">gridDim * blockDim</code> coming from?</div>
<p>If you look at the bullet points above, each element processed by a thread is separated by 1000 indices. 1000 is just the number of threads running: <code class="article-code-inline">gridDim * blockDim = 2 * 500 = 1000</code>.</p>
<p>It is as if each thread was looping around and processing the <span class="article-bold">same element</span> of a <span class="article-bold">new dataset</span>. That is, if we had 1000 elements to process, then thread <code class="article-code-inline">0</code> takes element <code class="article-code-inline">0</code>, thread <code class="article-code-inline">1</code> takes element <code class="article-code-inline">1</code>, etc. But here we have 4 more groups of 1000 elements to process. Thread <code class="article-code-inline">0</code> will process the <span class="article-italic">first</span> element of each group, which, if you flatten the indices are: <code class="article-code-inline">0, 1000, 2000, 3000, 4000</code>.</p>
<h2 id=grid-dim class="article-section">Grid Dim</h2>
<p>So far, we've been setting the grid dimension manually, hopefully you now understand that we can compute it based on the number of elements to process as long as we fix the block size.</p>
<p>For example, if the block size is fixed to <code class="article-code-inline">block_dim=1024</code>, and we have <code class="article-code-inline">n</code> elements to process, then we're looking for the <span class="article-bold">smallest</span> <code class="article-code-inline">grid_dim</code> such that:</p>
<div class="article-latex">[begin-latex]\begin{align*}
& \text{grid dim} * \text{block dim} \geq n \\[2em]
\iff & \text{grid dim} = \left\lceil\frac{n}{\text{block dim}}\right\rceil \\[2em]
\iff & \text{grid dim} = \frac{n + \text{block dim} - 1}{\text{block dim}}
\end{align*}[end-latex]</div>
<p>For example, if we have 1,000,000 elements to process and we fix <code class="article-code-inline">block_dim=1024</code>, then <code class="article-code-inline">grid_dim=ceil(1000000 / 1024) = 977</code>.</p>
<h2 id=profiling class="article-section">Profiling</h2>
<p>If you give the compiled binary to the NVIDIA profiler <code class="article-code-inline">nvprof</code>, it will measure the time it took to execute the kernel, which is super useful to compare performances.</p>
<p>In the Colab cell:</p>
<pre class="article-code-block shell"><code>!nvcc -arch=sm_75 main.cu -o main
!nvprof ./main</code></pre>
<p>Will output something like:</p>
<pre class="article-code-block shell"><code>==1086== NVPROF is profiling process 1086, command: ./main
Errors = 0
==1086== Profiling application: ./main
==1086== Profiling result:
            Type  Time(%)      Time     Calls       Avg       Min       Max  Name
 GPU activities:   44.98%  3.8720us         1  3.8720us  3.8720us  3.8720us  add_kernel(float*, float*, float*, int)
                   28.25%  2.4320us         2  1.2160us     992ns  1.4400us  [CUDA memcpy HtoD]
                   26.77%  2.3040us         1  2.3040us  2.3040us  2.3040us  [CUDA memcpy DtoH]
      API calls:   99.77%  187.09ms         3  62.363ms  2.9720us  187.08ms  cudaMalloc
                    0.07%  137.48us       114  1.2050us     107ns  53.157us  cuDeviceGetAttribute
                    0.06%  116.14us         3  38.712us  3.4600us  106.42us  cudaFree
                    0.06%  109.43us         1  109.43us  109.43us  109.43us  cudaLaunchKernel
                    0.03%  46.972us         3  15.657us  7.4770us  20.285us  cudaMemcpy
                    0.01%  12.017us         1  12.017us  12.017us  12.017us  cuDeviceGetName
                    0.00%  6.6660us         1  6.6660us  6.6660us  6.6660us  cudaDeviceSynchronize
                    0.00%  6.3690us         1  6.3690us  6.3690us  6.3690us  cuDeviceGetPCIBusId
                    0.00%  1.8510us         3     617ns     144ns  1.3680us  cuDeviceGetCount
                    0.00%     939ns         2     469ns     140ns     799ns  cuDeviceGet
                    0.00%     466ns         1     466ns     466ns     466ns  cuModuleGetLoadingMode
                    0.00%     430ns         1     430ns     430ns     430ns  cuDeviceTotalMem
                    0.00%     257ns         1     257ns     257ns     257ns  cuDeviceGetUuid</code></pre>
<p>You can see the performance of every CUDA library call, and more specifically:</p>
<pre class="article-code-block None"><code>Time(%)      Time     Calls       Avg       Min       Max  Name
 44.98%  3.8720us         1  3.8720us  3.8720us  3.8720us  add_kernel(float*, float*, float*, int)</code></pre>
<p>It took 3 micro seconds to run the add_kernel on 1000 elements.</p>
<p>Now, let's actually bump this number to 1 million and do some profiling!</p>
<h2 id=summary-and-profiling-on-1m-elements class="article-section">Summary and Profiling on 1M elements</h2>
<p>We can actually use the grid-stride loop version of our kernel for all tests, since this adapts to any number of threads and blocks.</p>
<pre class="article-code-block cpp"><code>__global__ void add_kernel(float *array1, float *array2, float *result, int size)
{
    int start = blockDim.x * blockIdx.x + threadIdx.x;
    int stride = gridDim.x * blockDim.x;

    for (int i=start; i&lt;size; i+=stride) {
        result[i] = array1[i] + array2[i];
    }
}</code></pre>
<p>I'm building <code class="article-code-inline">array1</code> and <code class="article-code-inline">array2</code> with <code class="article-code-inline">int size = 1 &lt;&lt; 20</code>, which is approximately 1 million elements.</p>
<h3 id=sequential-126ms class="article-small-section">Sequential ~126ms</h3>
<pre class="article-code-block cpp"><code>add_kernel&lt;&lt;&lt;1, 1&gt;&gt;&gt;(cuda_array1, cuda_array2, cuda_result, size);</code></pre>
<p>Profiling this gives us:</p>
<pre class="article-code-block shell"><code>Time(%)      Time     Calls       Avg       Min       Max  Name
 97.27%  126.01ms         1  126.01ms  126.01ms  126.01ms  add_kernel(float*, float*, float*, int)</code></pre>
<h3 id=one-block-682us class="article-small-section">One block ~682us</h3>
<pre class="article-code-block cpp"><code>add_kernel&lt;&lt;&lt;1, 1024&gt;&gt;&gt;(cuda_array1, cuda_array2, cuda_result, size);</code></pre>
<p>Profiling gives us:</p>
<pre class="article-code-block shell"><code>Time(%)      Time     Calls       Avg       Min       Max  Name
 18.33%  682.84us         1  682.84us  682.84us  682.84us  add_kernel(float*, float*, float*, int)</code></pre>
<h3 id=multiple-blocks-76us class="article-small-section">Multiple blocks ~76us</h3>
<pre class="article-code-block cpp"><code>int block_dim = 1024;
int grid_dim = (size + block_dim - 1) / block_dim;
add_kernel&lt;&lt;&lt;grid_dim, block_dim&gt;&gt;&gt;(cuda_array1, cuda_array2, cuda_result, size);</code></pre>
<p>Profiling gives us:</p>
<pre class="article-code-block shell"><code>Time(%)      Time     Calls       Avg       Min       Max  Name
  2.46%  76.127us         1  76.127us  76.127us  76.127us  add_kernel(float*, float*, float*, int)</code></pre>
<p>This runs <span class="article-bold">1657x</span> faster than doing the sum sequentially!</p>
<h2 id=dim3 class="article-section">Dim3</h2>
<p>I will conclude this article by explaining why <code class="article-code-inline">gridDim</code> and <code class="article-code-inline">blockDim</code> are <code class="article-code-inline">dim3</code> objects with <code class="article-code-inline">x, y, z</code> components.</p>
<p>These are abstractions over the idea of threads and blocks. So far we've used threads and blocks available in the <code class="article-code-inline">x</code> dimension, but you actually have 2 more to use. You would typically use those if your <span class="article-bold">problem is intrinsically 2d or 3d</span>.</p>
<p>Adding two lists of numbers only requires a single index, so that is 1d. However, processing matrices could be done more naturally with 2 indices (row, column). Everything we've seen in 1d applies in 2d and 3d.</p>
<p>At the beginning of the article we printed out the GPU specs:</p>
<pre class="article-code-block shell"><code>#   Max block dimensions: [1024, 1024, 64]
#   Max grid dimensions: [2147483647, 65535, 65535]</code></pre>
<p>Those three numbers correspond to the limits for the <code class="article-code-inline">x, y, z</code> dimensions. There are 2 important considerations:</p>
<ul><li class="article-li">Total threads per block. We saw the limit was 1024; this limit applies <span class="article-bold">across dimensions</span>. Meaning you can use <code class="article-code-inline">[32, 32, 1]</code> threads for <code class="article-code-inline">x, y, z</code> and that would be a valid configuration since <code class="article-code-inline">32 * 32 * 1 = 1024</code> which does not exceed the limit. On the other hand <code class="article-code-inline">[32, 32, 32]</code> is invalid.</li>
<li class="article-li">Per-dimension limit. Each of <code class="article-code-inline">x, y, z</code> must not contain more threads than the individual limits <code class="article-code-inline">[1024, 1024, 64]</code>. For example <code class="article-code-inline">[1025, 0, 0]</code> is invalid.</li></ul>
    </article>
    <div class="article-hr"></div>
    <div class="site-footer">
        <div class="site-footer-content"></div>
    </div>
    
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.js" integrity="sha512-LQNxIMR5rXv7o+b1l8+N1EZMfhG7iFZ9HhnbJkTp4zjNr5Wvst75AqUeFDxeRUa7l5vEDyUiAip//r+EFLLCyA==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/contrib/auto-render.min.js" integrity="sha512-iWiuBS5nt6r60fCz26Nd0Zqe0nbk1ZTIQbl3Kv7kYsX+yKMUFHzjaH2+AnM6vp2Xs+gNmaBAVWJjSmuPw76Efg==" crossorigin="anonymous" referrerpolicy="no-referrer" onload="renderMathInElement(document.body, {
    delimiters: [
        {left: '[begin-latex]', right: '[end-latex]', display: true},
        {left: '[begin-latex-inline]', right: '[end-latex-inline]', display: false},
    ]
});"></script>


<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.11.1/highlight.min.js" integrity="sha512-EBLzUL8XLl+va/zAsmXwS7Z2B1F9HUHkZwyS/VKwh3S7T/U0nF4BaU29EP/ZSf6zgiIxYAnKLu6bJ8dqpmX5uw==" crossorigin="anonymous" referrerpolicy="no-referrer" onload="hljs.highlightAll();"></script>

</body>

</html>