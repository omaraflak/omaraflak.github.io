<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta property="og:title" content="Neural Network From Scratch">
    <meta property="og:description" content="Build your own machine learning library in Python">
    <title>Neural Network From Scratch</title>
    <link rel="icon" type="image/svg+xml" href="/images/favicon.svg">
    <link rel="stylesheet" href="../styles/index.css">
    <link rel="stylesheet" href="../styles/fonts.css">
    
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css" integrity="sha512-fHwaWebuwA7NSF5Qg/af4UeDx9XqUpYpOGgubo3yWu+b2IQR4UeQwbb42Ti7gVAjNtVoI/I9TEoYeu9omwcC6g==" crossorigin="anonymous" referrerpolicy="no-referrer" />


<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.11.1/styles/github.min.css" integrity="sha512-0aPQyyeZrWj9sCA46UlmWgKOP0mUipLQ6OZXu8l4IcAmD2u31EPEy9VcIMvl7SoAaKe8bLXZhYoMaE/in+gcgA==" crossorigin="anonymous" referrerpolicy="no-referrer" />

</head>

<body>
    <div class="site-header">
        <div class="site-header-content">
            <a href="/index.html" class="site-header-link">Home</a>
            <a href="/articles.html" class="site-header-link">Blog</a>
        </div>
    </div>
    <div class="article-header">
        <div class="article-header-content">
            <h1 class="article-title">Neural Network From Scratch</h1>
            <p class="article-description">Build your own machine learning library in Python</p>
            <p class="article-date">Nov 15, 2018</p>
            <span class="article-updated-date">Updated on Aug 03, 2025</span>
        </div>
    </div>
    <article class="article-body">
        <p>In this post we will go through the mathematics of neural networks and code from scratch, in Python, a small modular library to build and train such models. The only import we'll ever need is NumPy for dealing with n-dimensional arrays:</p>
<pre class="article-code-block python"><code>import numpy as np</code></pre>
<p>I will assume you already have some knowledge about neural networks. The purpose here is not to explain <span class="article-italic">why</span>, but <span class="article-italic">how</span>.</p>
<p>You can find the entire code of the article on my GitHub:</p>

            <div class="article-link-preview-container">
                <a class="article-link-preview-link" target="_blank" href="https://github.com/omaraflak/neural-network-from-scratch">
                    <span class="article-link-preview-title">GitHub - omaraflak/neural-network-from-scratch: Code for my article on omaraflak.github.io</span>
                    <span class="article-link-preview-description">Code for my article on omaraflak.github.io. Contribute to omaraflak/neural-network-from-scratch development by creating an account on GitHub.</span>
                    <span class="article-link-preview-website">github.com</span>
                </a>
            </div>
        
<h2 id=preamble class="article-section">Preamble</h2>
<p>A neural network is nothing but a mathematical function. It's a function with generally many <span class="article-italic">parameters</span> which can be tweaked to change the behavior of that function. For example:</p>
<div class="article-latex">[begin-latex]f(x) = ax+b[end-latex]</div>
<p>This is the equation of a line, where [begin-latex-inline]a[end-latex-inline] and [begin-latex-inline]b[end-latex-inline] are the parameters that will determine which line exactly this equation represents. In the same way, a neural network is a very generic function that maps [begin-latex-inline]n[end-latex-inline] inputs to [begin-latex-inline]m[end-latex-inline] outputs and has some number of parameters, usually denoted as [begin-latex-inline]W[end-latex-inline], for <span class="article-italic">weights</span>.</p>
<div class="article-latex">[begin-latex]\begin{align*}
f_W: \quad &\R^n \rightarrow \R^m \\
&x \mapsto f_W(x)
\end{align*}[end-latex]</div>
<p>Those weights determine what the function does. A certain set of [begin-latex-inline]W[end-latex-inline] will make the function turn an image of a cat or dog (images are just pixels, which are just numbers) into a label 0 for cats and a label 1 for dogs. Another set of [begin-latex-inline]W[end-latex-inline] could make the function predict the next word in a sentence, etc.</p>
<p>So it's really a matter of finding the [begin-latex-inline]W[end-latex-inline] that makes the function behave the way you want. For this, we need 2 things: math and data.</p>
<p>In a nutshell, the idea is to define some <span class="article-bold"><span class="article-italic">loss function</span></span> that evaluates how bad the network prediction is compared to what you really wanted, and then try to minimize that function using <a class="article-link" target="_blank" href="https://en.wikipedia.org/wiki/Gradient_descent">Gradient Descent</a>.</p>
<p>In pseudo-Python code, that would be:</p>
<pre class="article-code-block python"><code>for _ in range(1000):
    y_pred = f(x, weights) # predict output from input
    error = loss(y_true, y_pred) # compute loss between actual output and desired output
    weights_gradient = error.gradient_wrt(weights) # compute derivative of error with respect to weights
    weights -= learning_rate * weights_gradient # update weights such that error is minimized (gradient descent)</code></pre>
<h2 id=layer-by-layer class="article-section">Layer by Layer</h2>
<p>Our neural network [begin-latex-inline]f_W[end-latex-inline] is usually a composition of many other small functions, i.e. [begin-latex-inline]f_W = f_1(f_2(f_3(x)))[end-latex-inline], each with its own set of parameters.</p>
<p>Those intermediate [begin-latex-inline]f_1[end-latex-inline], [begin-latex-inline]f_2[end-latex-inline], [begin-latex-inline]f_3[end-latex-inline] functions are what we call <span class="article-bold"><span class="article-italic">layers</span></span>. Since they're a fundamental building block of neural network, we want to model them properly using an abstract class.</p>
<h3 id=forward-propagation class="article-small-section">Forward Propagation</h3>
<p>All layers must be able to return an output, given a certain input:</p>
<div class="article-latex">[begin-latex]X \rightarrow \boxed{\text{Layer}} \rightarrow Y[end-latex]</div>
<p>We can already emphasize one important point which is: since our neural network is a composition of functions, the output of one layer is the input of the next one.</p>
<div class="article-latex">[begin-latex]X \rightarrow \boxed{\text{Layer 1}} \rightarrow H_1 \rightarrow \boxed{\text{Layer 2}} \rightarrow H_2 \rightarrow \boxed{\text{Layer 3}} \rightarrow Y \rightarrow E(Y^*, Y)[end-latex]</div>
<h3 id=backward-propagation class="article-small-section">Backward Propagation</h3>
<p>Remember that once the forward propagation is complete and we have calculated [begin-latex-inline]E(Y^*, Y)[end-latex-inline], we need each layer to update its parameters. This can only be done if each layer can compute [begin-latex-inline]\frac{\partial E}{\partial W}[end-latex-inline] if [begin-latex-inline]W[end-latex-inline] is the set of parameters of that layer.</p>
<p>Therefore, during backward propagation, when want a layer to be able to return [begin-latex-inline]\frac{\partial E}{\partial X}[end-latex-inline] given [begin-latex-inline]\frac{\partial E}{\partial W}[end-latex-inline]:</p>
<div class="article-latex">[begin-latex]\frac{\partial E}{\partial X} \leftarrow \boxed{\text{Layer}} \leftarrow \frac{\partial E}{\partial Y}[end-latex]</div>
<p>Here, I'm abusing the notation of derivatives, because we normally derive with respect to a variable, not a vector. However [begin-latex-inline]X \in \R^n[end-latex-inline] and [begin-latex-inline]Y \in \R^m[end-latex-inline]. This notation should be read as:</p>
<div class="article-latex">[begin-latex]\frac{\partial E}{\partial X} =
\begin{bmatrix}
\dfrac{\partial E}{\partial x_1} \\[3ex]
\dfrac{\partial E}{\partial x_2} \\[3ex]
\vdots \\[3ex]
\dfrac{\partial E}{\partial x_n}
\end{bmatrix}[end-latex]</div>
<div class="article-quote">Why do we want the layer to map [begin-latex-inline]\frac{\partial E}{\partial Y}[end-latex-inline] to [begin-latex-inline]\frac{\partial E}{\partial X}[end-latex-inline] ?</div>
<p>Forget about [begin-latex-inline]\frac{\partial E}{\partial X}[end-latex-inline] for now. The reason we want this, is that if we have access to [begin-latex-inline]\frac{\partial E}{\partial Y}[end-latex-inline] then we can very easily calculate [begin-latex-inline]\frac{\partial E}{\partial W}[end-latex-inline] (for gradient descent) without knowing anything about the network architecture! We simply use the chain rule:</p>
<div class="article-latex">[begin-latex]\frac{\partial E}{\partial W} = \frac{\partial E}{\partial Y} \frac{\partial Y}{\partial W}[end-latex]</div>
<p>Another abuse of notation... What I really mean is:</p>
<div class="article-latex">[begin-latex]\frac{\partial E}{\partial w} = \sum_i \frac{\partial E}{\partial y_i} \frac{\partial y_i}{\partial w}[end-latex]</div>
<p>Since [begin-latex-inline]\frac{\partial E}{\partial y_i}[end-latex-inline] would be given, the layer simply needs to calculate [begin-latex-inline]\frac{\partial y_i}{\partial w}[end-latex-inline] which it knows how to, since for the layer: [begin-latex-inline]Y=f_W(X)[end-latex-inline].</p>
<div class="article-quote">Okay, what about [begin-latex-inline]\frac{\partial E}{\partial X}[end-latex-inline] ?</div>
<p>Remember that the output of one layer is the input to the next one! Which means returning [begin-latex-inline]\frac{\partial E}{\partial X}[end-latex-inline] from a given layer, will give the previous layer its own [begin-latex-inline]\frac{\partial E}{\partial Y}[end-latex-inline]. That is how we propagate the gradient through the network. We can again use the chain rule:</p>
<div class="article-latex">[begin-latex]\frac{\partial E}{\partial x_i} = \sum_j \frac{\partial E}{\partial y_j} \frac{\partial y_j}{\partial x_i}[end-latex]</div>
<p>This is really the <span class="article-italic">key</span> to understand backpropagation!</p>
<div class="article-latex">[begin-latex]\xleftarrow{\frac{\partial E}{\partial X_{}}} \boxed{\text{Layer 1}} \xleftarrow{\frac{\partial E}{\partial H_1}} \boxed{\text{Layer 2}} \xleftarrow{\frac{\partial E}{\partial H_2}} \boxed{\text{Layer 3}} \xleftarrow{\frac{\partial E}{\partial Y_{}}}[end-latex]</div>
<h3 id=abstract-base-class-module class="article-small-section">Abstract Base Class: Module</h3>
<pre class="article-code-block python"><code>class Module:
    &quot;&quot;&quot;Abstract class for a module in a neural network.&quot;&quot;&quot;

    def __init__(self):
        self.inputs: np.ndarray = None
        self.outputs: np.ndarray = None

    def forward(self, inputs: np.ndarray) -&gt; np.ndarray:
        raise NotImplementedError()

    def backward(self, output_grad: np.ndarray) -&gt; np.ndarray:
        raise NotImplementedError()

    def parameters(self) -&gt; list[np.ndarray]:
        return []

    def gradients(self) -&gt; list[np.ndarray]:
        return []</code></pre>
<p>As expected we have a <code class="article-code-inline">forward()</code> and a <code class="article-code-inline">backward()</code> method, but we also have two extra methods for the <code class="article-code-inline">parameters()</code> and the <code class="article-code-inline">gradients()</code> of those parameters. This is because it won't be layer itself that will do the update, but a dedicated training routine. The layer will simply <span class="article-bold">accumulated</span> the weight gradients during the backward method (<code class="article-code-inline">+=</code>) and should return gradient in the same order as it provides the parameters.</p>
<p>Now let's implement some layers!</p>
<h2 id=dense-layer class="article-section">Dense Layer</h2>
<p>The dense layer is one of the fundamental pieces in neural networks. You'll often find it represented that way:</p>
<center><div class="article-graphviz">digraph NeuralNetworkLayer {
    rankdir=LR;
    splines=line;
    bgcolor="transparent";

    {
        rank=same;
        node [shape=plaintext, style="", fontsize=16];
        in_1 [label="x1"];
        in_2 [label="x2"];
        in_n [label="xn"];
    }

    {
        rank=same;
        node [shape=circle, fillcolor="#DDEBFF", style=filled, label="", fixedsize=true, width=0.8];
        x1; x2; xn;
    }

    {
        rank=same;
        node [shape=circle, fillcolor="#FFDDDD", style=filled, label="", fixedsize=true, width=0.8];
        y1; y2; y3; ym;
    }

    {
        rank=same;
        node [shape=plaintext, style="", fontsize=16];
        out_1 [label="y1"];
        out_2 [label="y2"];
        out_3 [label="y3"];
        out_m [label="ym"];
    }

    edge [color=grey];
    {x1, x2, xn} -> {y1, y2, y3, ym};

    edge [color=black];
    in_1 -> x1;
    in_2 -> x2;
    in_n -> xn;

    y1 -> out_1;
    y2 -> out_2;
    y3 -> out_3;
    ym -> out_m;
}</div></center>
<p>Where this means to say that each input <span class="article-italic">"neuron"</span> is connected to each output <span class="article-italic">"neuron"</span>.</p>
<h3 id=forward-propagation class="article-small-section">Forward Propagation</h3>
<p>The actual value of each output neuron is calculated as the following:</p>
<div class="article-latex">[begin-latex]y_j = b_j + \sum_i x_i w_{ji}[end-latex]</div>
<p>Where [begin-latex-inline]w_{ji}[end-latex-inline] and [begin-latex-inline]b_j[end-latex-inline] are <span class="article-italic">learnable</span> parameters, commonly called <span class="article-italic">weights</span> and <span class="article-italic">biases</span>.</p>
<p>As you can see, this is just a linear transformation, so it can be expressed more succintly for all output neurons as a simple matrix multiplication:</p>
<div class="article-latex">[begin-latex]\begin{bmatrix}
y_1 \\
y_2 \\
\vdots \\
y_m
\end{bmatrix} = 
\begin{bmatrix}
w_{11} & w_{12} & \cdots & w_{1n} \\
w_{21} & w_{22} & \cdots & w_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
w_{m1} & w_{m2} & \cdots & w_{mn}
\end{bmatrix}
\begin{bmatrix}
x_1 \\
x_2 \\
\vdots \\
x_n
\end{bmatrix} +
\begin{bmatrix}
b_1 \\
b_2 \\
\vdots \\
b_m
\end{bmatrix}[end-latex]</div>
<div class="article-latex">[begin-latex]Y = WX + B[end-latex]</div>
<p>In effect, this is mapping [begin-latex-inline]n[end-latex-inline] inputs to [begin-latex-inline]m[end-latex-inline] outputs.</p>
<h3 id=backward-propagation class="article-small-section">Backward Propagation</h3>
<p>As we said earlier, suppose we have a matrix containing the derivative of the error with respect to that layer's output [begin-latex-inline]\frac{\partial E}{\partial Y}[end-latex-inline]. We need to compute:</p>
<ul><li class="article-li">The derivative of the error with respect to the parameters [begin-latex-inline]\frac{\partial E}{\partial W}[end-latex-inline], [begin-latex-inline]\frac{\partial E}{\partial B}[end-latex-inline]</li>
<li class="article-li">The derivative of the error with respect to the input [begin-latex-inline]\frac{\partial E}{\partial X}[end-latex-inline]</li></ul>
<p>Let's calculate [begin-latex-inline]\frac{\partial E}{\partial B}[end-latex-inline]. This matrix should be the same size as [begin-latex-inline]B[end-latex-inline] itself ([begin-latex-inline]m \times 1[end-latex-inline]):</p>
<div class="article-latex">[begin-latex]\frac{\partial E}{\partial B} =
\begin{bmatrix}
\dfrac{\partial E}{\partial b_1} \\[3ex]
\dfrac{\partial E}{\partial b_2} \\[3ex]
\vdots \\[3ex]
\dfrac{\partial E}{\partial b_m}
\end{bmatrix}[end-latex]</div>
<p>Using the chain rule:</p>
<div class="article-latex">[begin-latex]\begin{align*}
\frac{\partial E}{\partial b_i} &= \sum_j \frac{\partial E}{\partial y_j} \frac{\partial y_j}{\partial b_i} \\[3ex]
&= \frac{\partial E}{\partial y_i}
\end{align*}[end-latex]</div>
<p>Everything cancels out since the [begin-latex-inline]b_i[end-latex-inline] only appears in [begin-latex-inline]y_i[end-latex-inline]. Also, we can rewrite this more simply as:</p>
<div class="article-latex">[begin-latex]\frac{\partial E}{\partial B} = \frac{\partial E}{\partial Y}[end-latex]</div>
<p>Let's move on to the [begin-latex-inline]\frac{\partial E}{\partial W}[end-latex-inline], which should be the same size as [begin-latex-inline]W[end-latex-inline] itself ([begin-latex-inline]m \times n[end-latex-inline]):</p>
<div class="article-latex">[begin-latex]\frac{\partial E}{\partial W} =
\begin{bmatrix}
\dfrac{\partial E}{\partial w_{11}} & \dfrac{\partial E}{\partial w_{12}} & \cdots & \dfrac{\partial E}{\partial w_{1n}} \\[3ex]
\dfrac{\partial E}{\partial w_{21}} & \dfrac{\partial E}{\partial w_{22}} & \cdots & \dfrac{\partial E}{\partial w_{2n}} \\[3ex]
\vdots & \vdots & \ddots & \vdots \\[3ex]
\dfrac{\partial E}{\partial w_{m1}} & \dfrac{\partial E}{\partial w_{m2}} & \cdots & \dfrac{\partial E}{\partial w_{mn}}
\end{bmatrix}[end-latex]</div>
<p>Using the chain rule:</p>
<div class="article-latex">[begin-latex]\begin{align*}
\frac{\partial E}{\partial w_{ij}} &= \sum_k \frac{\partial E}{\partial y_k} \frac{\partial y_k}{\partial w_{ij}} \\[3ex]
&= \frac{\partial E}{\partial y_i} \frac{\partial y_i}{\partial w_{ij}} \\[3ex]
&= \frac{\partial E}{\partial y_i} x_j
\end{align*}[end-latex]</div>
<p>Subsitituting this result in the [begin-latex-inline]\frac{\partial E}{\partial W}[end-latex-inline] matrix:</p>
<div class="article-latex">[begin-latex]\begin{align*}
\frac{\partial E}{\partial W} &=
\begin{bmatrix}
\dfrac{\partial E}{\partial y_1} x_1 & \dfrac{\partial E}{\partial y_1} x_2 & \cdots & \dfrac{\partial E}{\partial y_1} x_n \\[3ex]
\dfrac{\partial E}{\partial y_2} x_1 & \dfrac{\partial E}{\partial y_2} x_2 & \cdots & \dfrac{\partial E}{\partial y_2} x_n \\[3ex]
\vdots & \vdots & \ddots & \vdots \\[3ex]
\dfrac{\partial E}{\partial y_m} x_1 & \dfrac{\partial E}{\partial y_m} x_2 & \cdots & \dfrac{\partial E}{\partial y_m} x_n
\end{bmatrix}
\\
\\
&=
\begin{bmatrix}
\dfrac{\partial E}{\partial y_1} \\[3ex]
\dfrac{\partial E}{\partial y_2} \\[3ex]
\vdots \\[3ex]
\dfrac{\partial E}{\partial y_m}
\end{bmatrix}
\begin{bmatrix}
x_1 & x_2 & \cdots & x_n
\end{bmatrix}
\\
\\
&= \frac{\partial E}{\partial Y} X^{\top}
\end{align*}[end-latex]</div>
<p>Lastly, we need to compute [begin-latex-inline]\frac{\partial E}{\partial X}[end-latex-inline] which will be passed to the previous layer during backpropagation!</p>
<p>Again, [begin-latex-inline]\frac{\partial E}{\partial X}[end-latex-inline] should be the same size as [begin-latex-inline]X[end-latex-inline] itself ([begin-latex-inline]n \times 1[end-latex-inline]):</p>
<div class="article-latex">[begin-latex]\frac{\partial E}{\partial X} =
\begin{bmatrix}
\dfrac{\partial E}{\partial x_1} \\[3ex]
\dfrac{\partial E}{\partial x_2} \\[3ex]
\vdots \\[3ex]
\dfrac{\partial E}{\partial x_n}
\end{bmatrix}[end-latex]</div>
<p>Using the chain rule:</p>
<div class="article-latex">[begin-latex]\begin{align*}
\frac{\partial E}{\partial x_j} &= \sum_i \frac{\partial E}{\partial y_i} \frac{\partial y_i}{\partial x_j} \\[3ex]
&= \sum_i \frac{\partial E}{\partial y_i} w_{ij}
\end{align*}[end-latex]</div>
<p>Substituting in [begin-latex-inline]\frac{\partial E}{\partial X}[end-latex-inline]:</p>
<div class="article-latex">[begin-latex]\begin{align*}
\frac{\partial E}{\partial X} &=
\begin{bmatrix}
\sum_i \dfrac{\partial E}{\partial y_i} w_{i1} \\[3ex]
\sum_i \dfrac{\partial E}{\partial y_i} w_{i2} \\[3ex]
\vdots \\[3ex]
\sum_i \dfrac{\partial E}{\partial y_i} w_{in}
\end{bmatrix}
\\
\\
&=
\begin{bmatrix}
\dfrac{\partial E}{\partial y_1} w_{11} + \dfrac{\partial E}{\partial y_2} w_{21} + \cdots + \dfrac{\partial E}{\partial y_m} w_{m1} \\[3ex]
\dfrac{\partial E}{\partial y_1} w_{12} + \dfrac{\partial E}{\partial y_2} w_{22} + \cdots + \dfrac{\partial E}{\partial y_m} w_{m2} \\[3ex]
\vdots \\[3ex]
\dfrac{\partial E}{\partial y_1} w_{1n} + \dfrac{\partial E}{\partial y_2} w_{2n} + \cdots + \dfrac{\partial E}{\partial y_m} w_{mn}
\end{bmatrix}
\\
\\
&=
\begin{bmatrix}
w_{11} & w_{21} & \cdots & w_{m1} \\
w_{12} & w_{22} & \cdots & w_{m2} \\
\vdots & \vdots & \ddots & \vdots \\
w_{1n} & w_{2n} & \cdots & w_{mn}
\end{bmatrix}
\begin{bmatrix}
\dfrac{\partial E}{\partial y_1} \\[3ex]
\dfrac{\partial E}{\partial y_2} \\[3ex]
\vdots \\[3ex]
\dfrac{\partial E}{\partial y_m}
\end{bmatrix}
\\
\\
&=
W^{\top} \frac{\partial E}{\partial Y}
\end{align*}[end-latex]</div>
<h3 id=dense-layer-code class="article-small-section">Dense Layer Code</h3>
<p>We can now create a Python class for the Dense layer:</p>
<pre class="article-code-block python"><code>class Linear(Module):
    &quot;&quot;&quot;Linear transformation applied to input column vector.&quot;&quot;&quot;

    def __init__(self, input_size: int, output_size: int):
        super().__init__()
        self.weights = np.random.randn(output_size, input_size)
        self.bias = np.random.randn(output_size, 1)
        self.weights_grad = np.zeros_like(self.weights)
        self.bias_grad = np.zeros_like(self.bias)

    def forward(self, inputs: np.ndarray) -&gt; np.ndarray:
        self.inputs = inputs
        self.outputs = np.dot(self.weights, inputs) + self.bias
        return self.outputs

    def backward(self, output_grad: np.ndarray) -&gt; np.ndarray:
        self.weights_grad += np.dot(output_grad, self.inputs.T)
        self.bias_grad += output_grad
        return np.dot(self.weights.T, output_grad)

    def parameters(self) -&gt; list[np.ndarray]:
        return [self.weights, self.bias]

    def gradients(self) -&gt; list[np.ndarray]:
        return [self.weights_grad, self.bias_grad]</code></pre>
<p>We have completed a milestone in our neural network library! The rest should be a lot easier!</p>
<p>So far, we are able to perform linear transformation on our data. However, the effect of applying many linear transformations in a row is nothing but a linear transformation itself! We need to add non-linearity in the neural network by applying non-linear functions to the data. This is where <span class="article-italic">activation layers</span> come in the picture.</p>
<h2 id=activation-layer class="article-section">Activation Layer</h2>
<p>An activation layer simply passes each of its inputs into a non-linear activation function that we define. Some commonly used activation functions are [begin-latex-inline]tanh(x)[end-latex-inline] or [begin-latex-inline]max(0, x)[end-latex-inline].</p>

            <div class="article-link-preview-container">
                <a class="article-link-preview-link" target="_blank" href="https://en.wikipedia.org/wiki/Activation_function">
                    <span class="article-link-preview-title">Activation function - Wikipedia</span>
                    <span class="article-link-preview-description"></span>
                    <span class="article-link-preview-website">en.wikipedia.org</span>
                </a>
            </div>
        
<center><div class="article-graphviz">digraph NeuralNetworkLayer {
    rankdir=LR;
    splines=line;
    bgcolor="transparent";

    {
        rank=same;
        node [shape=plaintext, style="", fontsize=16];
        in_1 [label="x1"];
        in_2 [label="x2"];
        in_n [label="xn"];
    }

    {
        rank=same;
        node [shape=circle, fillcolor="#DDEBFF", style=filled, label="", fixedsize=true, width=0.8];
        x1; x2; xn;
    }

    {
        rank=same;
        node [shape=circle, fillcolor="#FFDDDD", style=filled, label="", fixedsize=true, width=0.8];
        y1; y2; yn;
    }

    {
        rank=same;
        node [shape=plaintext, style="", fontsize=16];
        out_1 [label="y1"];
        out_2 [label="y2"];
        out_n [label="yn"];
    }

    edge [color=grey];
    x1 -> y1;
    x2 -> y2;
    xn -> yn;

    edge [color=black];
    in_1 -> x1;
    in_2 -> x2;
    in_n -> xn;

    y1 -> out_1;
    y2 -> out_2;
    yn -> out_n;

    edge [style=invis];
    x1 -> x2 -> xn;
}</div></center>
<h3 id=forward-propagation class="article-small-section">Forward Propagation</h3>
<p>For a given input [begin-latex-inline]X[end-latex-inline], the output is the activation function applied to every element of [begin-latex-inline]X[end-latex-inline].</p>
<div class="article-latex">[begin-latex]\begin{align*}
Y &= \begin{bmatrix}
f(x_1) \\
f(x_2) \\
\vdots \\
f(x_n)
\end{bmatrix}
\\
\\
&= f(X)
\end{align*}[end-latex]</div>
<h3 id=backward-propagation class="article-small-section">Backward Propagation</h3>
<p>Similarily as for the Dense layer, we are given [begin-latex-inline]\frac{\partial E}{\partial Y}[end-latex-inline] and we want to compute [begin-latex-inline]\frac{\partial E}{\partial X}[end-latex-inline]. This time, there's no no trainable parameters so that's all we need.</p>
<div class="article-quote">Actually some activation functions could have trainable parameters, but in that case it would be better to implement them in a separate layer. The layer we'll be creating now is a simple wrapper for [begin-latex-inline]\R \to \R[end-latex-inline] mapping functions without trainable parameters.</div>
<div class="article-latex">[begin-latex]\begin{align*}
\frac{\partial E}{\partial X} &=
\begin{bmatrix}
\dfrac{\partial E}{\partial x_1} \\[3ex]
\dfrac{\partial E}{\partial x_2} \\[3ex]
\vdots \\[3ex]
\dfrac{\partial E}{\partial x_n}
\end{bmatrix}
=
\begin{bmatrix}
\dfrac{\partial E}{\partial y_1} \dfrac{\partial E}{\partial x_1} \\[3ex]
\dfrac{\partial E}{\partial y_2} \dfrac{\partial E}{\partial x_2} \\[3ex]
\vdots \\[3ex]
\dfrac{\partial E}{\partial y_n} \dfrac{\partial E}{\partial x_n}
\end{bmatrix}
=
\begin{bmatrix}
\dfrac{\partial E}{\partial y_1} f'(x_1) \\[3ex]
\dfrac{\partial E}{\partial y_2} f'(x_2) \\[3ex]
\vdots \\[3ex]
\dfrac{\partial E}{\partial y_n} f'(x_n)
\end{bmatrix}
=
\begin{bmatrix}
\dfrac{\partial E}{\partial y_1} \\[3ex]
\dfrac{\partial E}{\partial y_2} \\[3ex]
\vdots \\[3ex]
\dfrac{\partial E}{\partial y_n}
\end{bmatrix}
\odot
\begin{bmatrix}
f'(x_1) \\
f'(x_2) \\
\vdots \\
f'(x_n)
\end{bmatrix}
\\
\\
&=
\frac{\partial E}{\partial Y} \odot f'(X)
\end{align*}[end-latex]</div>
<p>Here, [begin-latex-inline]\odot[end-latex-inline] denotes element-wise multiplication.</p>
<h3 id=activation-layer-code class="article-small-section">Activation Layer Code</h3>
<p>We can now create a Python class for the Activation layer:</p>
<pre class="article-code-block python"><code>class Activation(Module):
    &quot;&quot;&quot;Applies an activation function to the input, element wise.&quot;&quot;&quot;

    def activation(self, inputs: np.ndarray) -&gt; np.ndarray:
        raise NotImplementedError()

    def activation_prime(self, inputs: np.ndarray) -&gt; np.ndarray:
        raise NotImplementedError()

    def forward(self, inputs: np.ndarray) -&gt; np.ndarray:
        self.inputs = inputs
        return self.activation(inputs)

    def backward(self, output_grad: np.ndarray) -&gt; np.ndarray:
        return output_grad * self.activation_prime(self.inputs)</code></pre>
<p>And implement one such activation, for example [begin-latex-inline]tanh(x)[end-latex-inline]:</p>
<pre class="article-code-block python"><code>class Tanh(Activation):
    &quot;&quot;&quot;Hyperbolic tangent activation layer.&quot;&quot;&quot;

    def activation(self, inputs: np.ndarray) -&gt; np.ndarray:
        return np.tanh(inputs)

    def activation_prime(self, inputs: np.ndarray) -&gt; np.ndarray:
        return 1.0 - np.power(np.tanh(inputs), 2)</code></pre>
<h2 id=sequential-layer class="article-section">Sequential Layer</h2>
<p>Remember how we said that neural networks are often composition of functions?</p>
<div class="article-latex">[begin-latex]X \rightarrow \boxed{\text{Layer 1}} \rightarrow H_1 \rightarrow \boxed{\text{Layer 2}} \rightarrow H_2 \rightarrow \boxed{\text{Layer 3}} \rightarrow Y \rightarrow E(Y^*, Y)[end-latex]</div>
<p>The composition itself is of course also a function, so we could implement a <span class="article-italic">Sequential</span> layer who's only job is to do that composition! This will make the code super clean and will allow us to represent the entire neural network as a single layer.</p>
<pre class="article-code-block python"><code>class Sequential(Module):
    &quot;&quot;&quot;Sequential neural network.&quot;&quot;&quot;

    def __init__(self, modules: list[Module]):
        super().__init__()
        self.modules = modules

    def forward(self, inputs: np.ndarray) -&gt; np.ndarray:
        outputs = inputs
        for module in self.modules:
            outputs = module.forward(outputs)
        return outputs

    def backward(self, output_grad: np.ndarray) -&gt; np.ndarray:
        input_grad = output_grad
        for module in reversed(self.modules):
            input_grad = module.backward(input_grad)
        return input_grad

    def parameters(self) -&gt; list[np.ndarray]:
        return [param for module in self.modules for param in module.parameters()]

    def gradients(self) -&gt; list[np.ndarray]:
        return [grad for module in self.modules for grad in module.gradients()]</code></pre>
<p>As you can see, we now wrote very explicitly two ideas:</p>
<ul><li class="article-li">Forward propagation: the output of one layer is the input of the next one</li>
<li class="article-li">Backward propagation: the input gradient of one layer is the output gradient of the previous one</li></ul>
<p>With this class in hand, a neural network can be expressed as easily as this:</p>
<pre class="article-code-block python"><code># A neural network that maps 10 inputs to 3 outputs
network = Sequential([
    Linear(10, 50),
    Tanh(),
    Linear(50, 3),
    Tanh()
])</code></pre>
<h2 id=loss-function class="article-section">Loss Function</h2>
<p>Until now, for a given layer, we supposed that [begin-latex-inline]\frac{\partial E}{\partial Y}[end-latex-inline] was given by the next layer. But what happens to the last layer? How does it get the output gradient?</p>
<p>During the training loop, we will pass that gradient manually to the last layer and then propagate the value to all previous layers. However, we still haven't defined what [begin-latex-inline]E[end-latex-inline] is! This is the <span class="article-italic">loss function</span>.</p>
<p>Just like activation functions, there's a panoply of loss functions that you can pick from depending on your problem. In all cases, the loss is computed based on the output of the network and the true desired output.</p>
<p>For now, we will see one such function: <span class="article-bold">MSE</span>, or Mean Squared Error.</p>
<h3 id=mean-squared-error class="article-small-section">Mean Squared Error</h3>
<p>For a given output of the neural network [begin-latex-inline]Y = \{y_1, y_2, \ldots, y_n\}[end-latex-inline] and the expected output [begin-latex-inline]Y^* = \{y^*_1, y^*_2, \ldots, y^*_n\}[end-latex-inline], MSE is defined as the squared distance between the two points:</p>
<div class="article-latex">[begin-latex]E = \frac{1}{n} \sum_i (y_i^* - y_i)^2[end-latex]</div>
<p>This single number will tell us how good or bad the model is doing and will eventually steer the training of the neural network. When [begin-latex-inline]E=0[end-latex-inline] then both the predicted output and the expected output are equal. This is a suitable loss for regression tasks, for example predicting stock prices.</p>
<p>At last, let's compute [begin-latex-inline]\frac{\partial E}{\partial Y}[end-latex-inline]:</p>
<div class="article-latex">[begin-latex]\begin{align*}
\frac{\partial E}{\partial Y} &=
\begin{bmatrix}
\dfrac{\partial E}{\partial y_1} \\[3ex]
\dfrac{\partial E}{\partial y_2} \\[3ex]
\vdots \\[3ex]
\dfrac{\partial E}{\partial y_n}
\end{bmatrix}
=
\begin{bmatrix}
\dfrac{2}{n} (y^*_1 - y_1)(-1) \\[3ex]
\dfrac{2}{n} (y^*_2 - y_2)(-1) \\[3ex]
\vdots \\[3ex]
\dfrac{2}{n} (y^*_n - y_n)(-1)
\end{bmatrix}
\\
\\
&=
\frac{2}{n} (Y - Y^*)
\end{align*}[end-latex]</div>
<h3 id=mean-squared-error-code class="article-small-section">Mean Squared Error Code</h3>
<p>Since the idea of a loss function is generic we'll have a base class and one implementation of it for MSE.</p>
<pre class="article-code-block python"><code>class Loss:
    &quot;&quot;&quot;Abstract class for a loss function.&quot;&quot;&quot;

    def loss(self, y_true: np.ndarray, y_pred: np.ndarray) -&gt; float:
        raise NotImplementedError()

    def loss_prime(self, y_true: np.ndarray, y_pred: np.ndarray) -&gt; np.ndarray:
        raise NotImplementedError()


class MSE(Loss):
    &quot;&quot;&quot;Mean squared error loss.&quot;&quot;&quot;

    def loss(self, y_true: np.ndarray, y_pred: np.ndarray) -&gt; float:
        return np.mean(np.power(y_true - y_pred, 2))

    def loss_prime(self, y_true: np.ndarray, y_pred: np.ndarray) -&gt; np.ndarray:
        return 2.0 * (y_pred - y_true) / np.size(y_true)</code></pre>
<h2 id=optimizer class="article-section">Optimizer</h2>
<p>So far we have built all the pieces that allow us to make predictions with our neural networks, compute gradients in the network, evaluate a given loss function, but we're never updating the parameters of the network to decrease the error. This too can be abstracted into a general concept: the <span class="article-bold"><span class="article-italic">optimizer</span></span>.</p>
<p>The optimizer's job is to update the parameters of the network using their associated gradients. Since the network itself is abstracted away as a <code class="article-code-inline">Module</code>, the optimizer can take that as a parameter. The <code class="article-code-inline">step()</code> function is where the update logic happens.</p>
<pre class="article-code-block python"><code>class Optimizer:
    &quot;&quot;&quot;Abstract class for updating the parameters of a module.&quot;&quot;&quot;

    def __init__(self, module: Module):
        self.module = module

    def step(self):
        raise NotImplementedError()

    def zero_gradients(self):
        for grad in self.module.gradients():
            grad.fill(0)</code></pre>
<h3 id=stocastic-gradient-descent class="article-small-section">Stocastic Gradient Descent</h3>
<p>One such optimizer is <span class="article-bold"><span class="article-italic">stocastic gradient descent</span></span>. For each parameter of the network it applies the following update:</p>
<div class="article-latex">[begin-latex]w_{t+1} = w_t - \alpha \frac{\partial E}{\partial w_t}, \quad \alpha > 0[end-latex]</div>
<p>[begin-latex-inline]\alpha[end-latex-inline] is a non-trainable scalar parameter that controls the update size. We call it the <span class="article-italic">learning rate</span>.</p>
<pre class="article-code-block python"><code>class SGD(Optimizer):
    def __init__(self, module: Module, learning_rate: float = 0.01):
        super().__init__(module)
        self.learning_rate = learning_rate

    def step(self):
        parameters = self.module.parameters()
        gradients = self.module.gradients()
        for i in range(len(parameters)):
            parameters[i] -= self.learning_rate * gradients[i]</code></pre>
<p><span class="article-bold">Note:</span> there are 3 variants of gradient descent which have different names:</p>
<ul><li class="article-li"><span class="article-bold">Gradient Descent</span>: the original method goes through the <span class="article-italic">entire</span> data, <span class="article-italic">accumulates</span> the gradients, and then does an update of the parameters (the average of the gradients at each step). This is slow in practice.</li>
<li class="article-li"><span class="article-bold">Stocastic Gradient Descent</span>: a variation where we do an update on the parameters after <span class="article-italic">every datapoint</span> seen. This is computationally expensive in practice.</li>
<li class="article-li"><span class="article-bold">Mini-Batch Gradient Descent</span>: a mix of 1 and 2 where we update the parameters after a predefined number of samples (mini-batch).</li></ul>
<p>The optimizer class is agnostic of when it's being called and therefore this logic will be implemented upstream, in the next section. This is why we have this <code class="article-code-inline">zero_gradients()</code> method which will help us reset the gradients <span class="article-bold"><span class="article-italic">when we want to</span></span> so we don't accumulate (<code class="article-code-inline">+=</code>) indefinitely. Although in practice, you almost certainly want to reset the gradients after an update.</p>
<h2 id=trainer class="article-section">Trainer</h2>
<p>We now have all the pieces we need to write the training routine of our neural networks! Let's put this in a function:</p>
<pre class="article-code-block python"><code>def train(
    module: Module,
    x_train: np.ndarray,
    y_train: np.ndarray,
    loss: Loss,
    optimizer: Optimizer,
    epochs: int,
) -&gt; list[float]:
    &quot;&quot;&quot;Trains the module on the given data, loss, and optimizer.&quot;&quot;&quot;
    errors = []
    for i in range(epochs):
        error = 0
        for x, y in zip(x_train, y_train):
            output = module.forward(x)
            error += loss.loss(y, output)
            module.backward(loss.loss_prime(y, output))
            optimizer.step()
            optimizer.zero_gradients()

        error /= len(x_train)
        errors.append(error)
        print(f&quot;{i+1}/{epochs} error={error:.5f}&quot;)
    return errors</code></pre>
<h2 id=xor class="article-section">XOR</h2>
<p>The <span class="article-italic">hello world</span> of neural networks is to solve XOR - Exclusive Or.</p>
<p>XOR (often denoted as [begin-latex-inline]\oplus[end-latex-inline]) is a function that maps [begin-latex-inline]\{0, 1\} \to \{0, 1\}[end-latex-inline] in the following way:</p>
<div class="article-latex">[begin-latex]0 \oplus 0 = 0 \\
0 \oplus 1 = 1 \\
1 \oplus 0 = 1 \\
1 \oplus 1 = 0 \\[end-latex]</div>
<div class="article-quote">Why is this even a challenge?</div>
<p>Because the XOR function is non-linear:</p>
<center><div id="xor-plot" style="margin-top: 20px; margin-bottom: 20px;"></div></center>
<p>In other words, you can never draw a line that will separate points [begin-latex-inline]\{(0,0), (1,1)\}[end-latex-inline] from points [begin-latex-inline]\{(1,0), (0,1)\}[end-latex-inline].</p>
<p>Therefore, if the neural network correctly learns a function that behaves like XOR for the given on the graph above, then it has learned a non-linear function, which is a very good sign.</p>
<pre class="article-code-block python"><code>x_train = np.array([[0, 0], [0, 1], [1, 0], [1, 1]]).reshape((4, 2, 1))
y_train = np.array([[0], [1], [1], [0]]).reshape((4, 1, 1))

model = Sequential([
    Linear(2, 3), # Map 2 inputs of XOR to 3 temporary units
    Tanh(),       # Activate those 3 temporary units
    Linear(3, 1), # Maps 3 units to 1 output
    Tanh(),       # Activate the output
])

trainer.train(
    model,
    x_train,
    y_train,
    MSE(),
    SGD(model),
    epochs=1000,
)

for x in x_train:
    print(x.tolist(), model.forward(x).tolist())</code></pre>
<p>Output:</p>
<pre class="article-code-block shell"><code>1/1000 error=2.19182
2/1000 error=2.10703
3/1000 error=1.88604
...
998/1000 error=0.00022
999/1000 error=0.00022
1000/1000 error=0.00022
[[0], [0]] [[0.0006479610416383107]]
[[0], [1]] [[0.9820342750260771]]
[[1], [0]] [[0.9766218725199783]]
[[1], [1]] [[0.0010051941355460458]]</code></pre>
<p>The model has been trained successfully!</p>
<p>I am sure this is not a satisfying result after going through all that math. So let's solve some more interesting problems with our library... in <span class="article-bold">part 2</span>!</p>

            <div class="article-link-preview-container">
                <a class="article-link-preview-link" target="_blank" href="https://omaraflak.github.io/articles/neural-network-2.html">
                    <span class="article-link-preview-title">Neural Network From Scratch — Part 2</span>
                    <span class="article-link-preview-description">Train a model to recognize hand written digits.</span>
                    <span class="article-link-preview-website">omaraflak.github.io</span>
                </a>
            </div>
        
<script src="https://cdnjs.cloudflare.com/ajax/libs/function-plot/1.25.1/function-plot.min.js" integrity="sha512-fsvE52IC5bx7NhuaGLoNE+Sq3EKFQ+fcvaJPE5hGemvMwQudqQuNXC4eG/8CjU2a90P88NzYPRl77iOcXerCHg==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>
<script src="/assets/neural-network/xor.js"></script>
    </article>
    <div class="article-hr"></div>
    <div class="site-footer">
        <div class="site-footer-content"></div>
    </div>
    
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.js" integrity="sha512-LQNxIMR5rXv7o+b1l8+N1EZMfhG7iFZ9HhnbJkTp4zjNr5Wvst75AqUeFDxeRUa7l5vEDyUiAip//r+EFLLCyA==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/contrib/auto-render.min.js" integrity="sha512-iWiuBS5nt6r60fCz26Nd0Zqe0nbk1ZTIQbl3Kv7kYsX+yKMUFHzjaH2+AnM6vp2Xs+gNmaBAVWJjSmuPw76Efg==" crossorigin="anonymous" referrerpolicy="no-referrer" onload="renderMathInElement(document.body, {
    delimiters: [
        {left: '[begin-latex]', right: '[end-latex]', display: true},
        {left: '[begin-latex-inline]', right: '[end-latex-inline]', display: false},
    ]
});"></script>


<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.11.1/highlight.min.js" integrity="sha512-EBLzUL8XLl+va/zAsmXwS7Z2B1F9HUHkZwyS/VKwh3S7T/U0nF4BaU29EP/ZSf6zgiIxYAnKLu6bJ8dqpmX5uw==" crossorigin="anonymous" referrerpolicy="no-referrer" onload="hljs.highlightAll();"></script>


<script defer src="https://cdnjs.cloudflare.com/ajax/libs/viz.js/2.1.2/viz.js" integrity="sha512-vnRdmX8ZxbU+IhA2gLhZqXkX1neJISG10xy0iP0WauuClu3AIMknxyDjYHEpEhi8fTZPyOCWgqUCnEafDB/jVQ==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/viz.js/2.1.2/lite.render.js" integrity="sha512-uAHj1knkgGpl0fJcyjbcVY0f9j252eWzEeBxE4s4AQkPJkp/+U+rlfoOXlwreSzPhndCT+5YR00/QSD/nPqb+g==" crossorigin="anonymous" referrerpolicy="no-referrer" onload="
    const viz = new Viz();
    document.querySelectorAll('.article-graphviz').forEach(element => {
        const content = element.textContent;
        try {
            viz.renderSVGElement(content)
                .then(svg => {
                    element.textContent = '';
                    element.appendChild(svg);
                })
                .catch(error => {
                    console.error('Error rendering Graphviz SVG:', error);
                    element.textContent = 'Error rendering graph. Check console for details.';
                });
        } catch (error) {
            console.error('Error with Viz.js rendering process:', error);
            element.textContent = 'Error during graph rendering setup.';
        }
    });
"></script>

</body>

</html>