:title: Entropy & Information
:description: What is information and how is it measured? What is entropy? Cross entropy? Relative entropy (aka KL Divergence)?
:year: 2025
:month: 2
:day: 21


# Information

Information is tied to the field of probabilities, and it can be seen as a measure of uncertainty. To avoid extrapolation and misuse of this concept, you need to remember that it only makes sense to talk about information (in the mathematical sense) when you are studying a probabilistic event.

> Information relates to probabilities in that the realisation of an event with low probability brings a lot of information, and the realisation of an event with high probability brings little information.

For example: the event “It rains in London” is very likely therefore it brings little information. In contrast, the event “It rains in the Sahara Desert” is so unlikely, it brings a lot more information (e.g. it could more realistically help you pin-point the day of the event).

## Theorem

$E=mc^2$
